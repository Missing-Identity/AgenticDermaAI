services:

  # ── Ollama model server ──────────────────────────────────────────────────────
  # Port 11434 is bound only to the loopback interface (127.0.0.1) so it is
  # never reachable from the public internet — only from the internal Docker
  # network (where the app container can reach it via hostname "ollama").
  ollama:
    image: ollama/ollama:latest
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      # Bind-mount the host's /root/.ollama directory so models stored on a
      # RunPod network volume (mounted at /root/.ollama on the host) survive
      # pod restarts. Falls back to a named Docker volume for local dev.
      - ${OLLAMA_MODELS_PATH:-ollama_data}:/root/.ollama
      # Mount the entrypoint script into the container
      - ./ollama-entrypoint.sh:/ollama-entrypoint.sh
    entrypoint: ["/bin/bash", "/ollama-entrypoint.sh"]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      # Only passes once Ollama is serving AND all models are listed
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      # start_period gives the model downloads time to finish before Docker
      # starts counting healthcheck failures. ~30 min for ~7 GB on a fast link.
      start_period: 30m
      retries: 3

  # ── FastAPI application ──────────────────────────────────────────────────────
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    env_file:
      - .env
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      # Persist uploaded images and generated PDF reports on the host
      - ./uploads:/app/uploads
      - ./reports:/app/reports
    restart: unless-stopped
    networks:
      - default

volumes:
  # Named volume so Ollama models survive container recreation
  ollama_data:
